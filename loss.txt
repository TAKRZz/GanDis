[1/10]: loss_d: 0.809, loss_g: 3.539
[2/10]: loss_d: 1.092, loss_g: 2.008
[3/10]: loss_d: 1.005, loss_g: 1.669
[4/10]: loss_d: 0.430, loss_g: 3.351
[5/10]: loss_d: 0.439, loss_g: 3.717
[6/10]: loss_d: 0.488, loss_g: 3.296
[7/10]: loss_d: 0.514, loss_g: 3.032
[8/10]: loss_d: 0.539, loss_g: 2.983
[9/10]: loss_d: 0.526, loss_g: 2.950
[10/10]: loss_d: 0.581, loss_g: 2.538

[1/10]: loss_d: 0.648, loss_g: 4.339
[2/10]: loss_d: 0.522, loss_g: 5.243
[3/10]: loss_d: 0.971, loss_g: 1.910
[4/10]: loss_d: 0.763, loss_g: 2.409
[5/10]: loss_d: 0.522, loss_g: 3.091
[6/10]: loss_d: 0.633, loss_g: 2.581
[7/10]: loss_d: 0.558, loss_g: 2.850
[8/10]: loss_d: 0.512, loss_g: 2.960
[9/10]: loss_d: 0.513, loss_g: 2.808
[10/10]: loss_d: 0.669, loss_g: 2.330
Takes : 621.47385597229s


Epoch 1499/1500
0.021180625637001627 0.01705069332886411 0.013533815054327936 0.002501252546238897 0.017429040090277126 0.000797483184708736 0.00985715843433177 7.578755841342044e-06 0.013475013999151297 0.016410564668985717 0.01319039481224027 0.025254928818243205 4.163985934946268e-07 0.01838903956556881 0.006542462946418914 0.014977381946194246 0.017003672819575377 0.010541248532863645 0.01482198020922565 0.0038553687244498036
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0
Epoch 1500/1500
0.02106591176480499 0.01342402319054814 0.013190519428954883 0.002957769375707353 0.01722261713131401 0.0006087107746709508 0.009793556208489096 2.4155063765851637e-06 0.013093559421777368 0.019689743669912052 0.014404823983125259 0.024440731156024853 2.2570796376353286e-07 0.017873525222730446 0.0069094672396895485 0.015563611415879364 0.01705917028141335 0.011269893691123523 0.014528053149481135 0.0027286060621026786
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0
best_num : 10
best_epoch : 746
def generator_model():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(12, input_shape=(8,), use_bias=False))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU())

    model.add(tf.keras.layers.Dense(14, use_bias=False))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU())

    model.add(tf.keras.layers.Dense(16, use_bias=False, activation='tanh'))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Reshape((16, 1)))
    return model


def discriminator_model():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Flatten())

    model.add(tf.keras.layers.Dense(32, use_bias=False))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU())

    model.add(tf.keras.layers.Dense(32, use_bias=False))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU())

    model.add(tf.keras.layers.Dense(1))
    return model


Epoch 531/1500
8.697910289634514e-05 0.002193098355733558 0.0020797462064047756 0.019437556512628573 0.034791356084009725 0.0018796390852925393 0.055850247777105524 2.1537291949869086e-09 0.01969822217676409 0.030390709264560667 0.005464570713880623 1.8695898030496316e-05 0.032199543449053236 0.028665818950640187 0.0022860091038404695 0.032591432396153475 9.952680903013711e-05 5.449196649465193e-12 3.244018570658369e-06 0.0010021262906332318 0.001684522036391467 0.002276815547735067 1.2352254045167044e-06 0.00897932133237897 0.00896604769737197 1.9413547247548735e-07 0.010461572995445922 3.10177471775086e-05 0.007673828466708166 0.01995189288552701 0.0 0.0 0.019113844537618108 0.002152995760917542 0.036056771462415704 6.661338147750939e-16 0.004373908592780751 0.04206003679910442 0.003435648851426776 0.03086069444654238
0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
def generator_model():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(6, input_shape=(4,), use_bias=False))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU())

    model.add(tf.keras.layers.Dense(8, use_bias=False))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU())

    model.add(tf.keras.layers.Dense(10, use_bias=False))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU())

    model.add(tf.keras.layers.Dense(13, use_bias=False, activation='tanh'))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.Reshape((13, 1)))
    return model


def discriminator_model():
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Flatten())

    model.add(tf.keras.layers.Dense(13, use_bias=False))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU())

    model.add(tf.keras.layers.Dense(8, use_bias=False))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU())

    model.add(tf.keras.layers.Dense(4, use_bias=False))
    model.add(tf.keras.layers.BatchNormalization())
    model.add(tf.keras.layers.LeakyReLU())

    model.add(tf.keras.layers.Dense(1))
    return model
Epoch 1500/1500
0.0018859421311868685 0.02802316919565523 3.553144598544833e-07 1.9562129693895258e-13 6.777272043567706e-08 0.015760508595424105 9.11521799151771e-08 0.01951110288925173 7.926650547052105e-08 0.010342270359087413 0.0053550972983612555 0.001604951326309001 0.012861006161705879 0.0006500838849090584 4.513409913919908e-06 0.0024211000152611906 0.0019057400900730492 0.00010419049694157523 6.055566524310052e-05 5.313527395855999e-13 0.01848930964010631 0.00524212625845899 3.434949631486717e-05 2.175138843707458e-06 0.03670051512068051 0.015505738010333725 8.726204436815266e-08 0.00841944551603091 0.016286412421113616 1.8456333772398636e-07 0.0004883192463963903 7.122778256096751e-09 7.667755319573644e-11 0.006924947606607668 9.611801831677091e-05 0.0024075895996825136 0.00021206414344598912 0.0010927042607090653 0.004729715271641011 0.0007110629745110675
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
best_num : 4
best_epoch : 531



Epoch 1500/1500
0.015563956325133188 0.00544184074701537 0.0071629726043395126 0.00809685270142535 0.007430527400672071 0.025455984775613894 0.0015829723973740384 0.007946657205828211 0.010966298276247177 0.00035231822707137983 0.007025165431773228 0.010617541152608934 0.004289928097534701 0.023924020993347117 0.02695921127867018 0.01413461951701045 0.00976793914585905 0.008286819779484311 0.03158006013408976 0.0004648583477674384 0.01120146499950292 0.009897384524452102 0.02414243864891652 0.01134529825992936 0.013305698696320811 0.0270669062143839 0.017912329830135243 0.011639927103632819 0.008576393202343824 0.01735120396025258 0.005913937879020059 0.004080230866345236 0.012694995991022306 0.014466486112223897 0.009137087289629475 0.005201848846031365 0.01140075984345379 0.015204114836181115 0.016410125129692088 0.005987735837555963
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
best_num : 3
best_epoch : 52


Epoch 1000/1000
0.0054415748109901685 0.004696336697460168 1.1888268147686176e-12 0.0 0.003028353650181906 6.975856349383802e-05 3.432106855627559e-05 0.04563545851471543 0.00023284895902997427 0.011333822053664733 0.0 3.7988612255901444e-10 0.00029629240109996235 0.00013837535773852938 0.0 0.0 0.023871338296728384 0.006473297758750052 0.0 0.004028132583177468 0.0036824558593258616 0.0005320298164904758 0.0028245631218700717 8.881784197001252e-16 0.004696336697460168 0.006456926304464705 0.000296007322768993 0.005155303053209437 0.010755691593760719 2.220446049250313e-15 3.192842364641013e-05 0.03284436187174922 3.4566508973710164e-05 0.008175688040147788 0.0007806947461915792 5.329070518200751e-15 0.04563545851471543 0.026849393916690792 0.006222413537746996 7.458162687490333e-05
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
best_num : 8
best_epoch : 907


Epoch 20000/20000
0.020861376119227337 0.014694307360317016 0.028663136499605946 0.0336403133801344 0.03143449126200265 0.027535871573761195 0.020861376119227337 0.032009736739806616 0.02909303210251213 0.02206107135092028 0.029681796744952194 0.03344649570018815 0.02909303210251213 0.01425965899238324 0.03340309937041985 0.03358963760128575 0.014694307360317016 0.02818079393818973 0.03189277872131635 0.020861376119227337 0.02003682471633006 0.0 0.020861376119227337 0.029890578172001137 0.014694307360317016 0.014694307360317016 0.031574692304531304 0.02003682471633006 0.020861376119227337 0.03340309937041985 0.020861376119227337 0.01678565162944412 0.020861376119227337 0.02818079393818973 0.027422497301741955 0.032722786687299776 0.031132257049544143 0.03210710586967458 0.028382053004534835 0.0
0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
best_num : 29
best_epoch : 4980